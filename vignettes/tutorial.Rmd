---
title: 'Tutorial'
output:
  rmarkdown::html_vignette:
    toc: false
    toc_depth: 4
    number_sections: false
bibliography: cttb.bib      
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
header-includes:  
    - \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    - \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
    - \usepackage{tikz}
    - \usetikzlibrary{positioning,shapes.geometric,chains,calc,shapes}
    - \usepackage{lmodern}
    - \usepackage{amssymb,amsmath}
    - \usepackage{ifxetex,ifluatex}
    - \usepackage{adjustbox} % adjust the size of table
    - \usepackage{mathtools}
    - \usepackage{threeparttable}
    - \usepackage{tabularx}
    - \usepackage{rotating}
    - \usepackage{fixltx2e}
    - \usepackage{bbm}
---
<!-- 
  Code to Justify Text
    <style>
    body {
    text-align: justify}
    </style>
-->   
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```  

In this tutorial, we demonstrate how to use the *CTTB* package in *R* for implementing the method of covariate-tightened trimming bounds proposed by @samii2023generalizing. This method enables empirical researchers to construct bounds for the local average treatment effect on a sub-population known as the always-responders in experiments or quasi-experiments, when faced with missing outcome or sample selection driven by potentially unobservable factors. 

## Problems with endogenous sample selection
Consider an experiment with $N$ units. For each unit $i$, we denote the outcome as $Y_i$, the binary treatment status as $D_i$, the binary response indicator as $S_i$, and $P$ pre-treatment covariates as $\mathbf{X}_i = (X_{1i}, X_{2i}, \dots, X_{Pi})$. The outcome $Y_i$ is observed only when $S_i = 1$. The variable $S_i$ reflects whether the unit opts to participate in the experiment or selects into reporting the outcome, and it may be influenced by the treatment status. Under the potential outcome framework, we have
$$
Y_i = \begin{cases}
Y_i(1), D_i = 1 \\
Y_i(0), D_i = 0,
\end{cases} \text{ and } S_i = \begin{cases}
S_i(1), D_i = 1 \\
S_i(0), D_i = 0.
\end{cases}
$$
For simplicity, we assume that the treatment is randomly assigned with a positive and known probability, hence
$$
\begin{aligned}
& (Y_i(1), Y_i(0), S_i(1), S_i(0)) \perp D_i, \\
& \varepsilon < P(D_i = 1) < 1 - \varepsilon, \text{ with } \varepsilon > 0.
\end{aligned}
$$
Note that we can only estimate the average outcome in either the treatment group or the control group conditional on the event $S_i = 1$. Since $S_i$ is not randomly assigned, the difference-in-means estimator may no longer be unbiased. As depicted in the Directed Acyclic Graph (DAG) below, there may exist an unobservable variable $U_i$ influencing both $S_i$ and $Y_i$. Then, $S_i$ becomes an endogenous variable and we have the problem of endogenous sample selection. The difference in the average outcome across the two groups now captures the influence of both $U_i$ and $D_i$, hence does not estimate the average treatment effect accurately.
```{tikz, echo=FALSE, fig.ext = 'png', out.width="60%", , out.height="60%", fig.align = 'center'}
\begin{tikzpicture}
[scale=0.7,dot/.style={fill,draw,circle,minimum width=1pt},
arrow style/.style={->,line width=1pt, shorten <=2pt,shorten >=2pt },
arrow2 style/.style={->,line width=2pt, shorten <=2pt,shorten >=2pt },]
\node [fill=white, dot, label=above: $S_{i}$] (s) at (3,3) {}; 
\node [fill=white, dot, label=below right: $Y_{i}$] (y) at (6,1) {} edge [arrow2 style] (s);
\node [fill=white, dot, label=below left: $D_{i}$] (d) at (1,1) {} edge [arrow style] (y) edge [arrow2 style] (s);
\node [fill=black, dot, label=above right: $U_{i}$] (u) at (6,3) {} edge [arrow2 style] (y) edge [arrow2 style] (s);
\end{tikzpicture}
```

Conventionally, to mitigate the bias arising from conditioning on $S_i$, researchers often turn to sample selection models [@heckman1979sample] or various imputation methods [@honaker2011amelia; @li2013weighting; @blackwell2017unified; @liu2021latent]. However, these methods generally assume that researchers have access to all variables influencing the selection process, a condition that is often unrealistic and contradicts the core rationale of conducting an experiment. 

## Introduction of the method
An alternative approach, which does not rely on such comprehensive knowledge, was proposed by @lee2009training. This method involves bounding the average treatment effect for a specific subgroup known as the always-respondersâ€”units whose response indicator remains at $1$ regardless of whether they are under treatment or control conditions. This quantity is formally defined as
$$
\tau(1, 1) = E[\tau_i | S_i(0) = S_i(1) = 1].
$$
It is sometimes referred to as the "intensive margin effect" in economics literature [@staub2014causal; @slough2022phantom]. From a policy standpoint, this particular subgroup represents units for whom the intervention poses minimal concerns about compelling them into an activity they may find undesirable. 

@lee2009training shows that we can bound $\tau(1, 1)$ under a much weaker assumption known as monotonic selection:
$$
S_i(1) \geq S_i(0) \text{ for any unit } i,
$$
which means that the treatment encourages all units to participate in the experiment and report the outcome. Obviously, we can also assume the opposition direction is true. This assumption is implied by all sample selection models but does not require us to know the actual selection process. When it is satisfied, @lee2009training proves that we can construct sharp bounds for $\tau(1, 1)$ as follows:
$$
\begin{aligned}
& \tau^L_{TB}(1, 1) \leq \tau(1, 1) \leq \tau^U_{TB}(1, 1), \text{ with} \\
& \tau^L_{TB}(1, 1) = E[Y_{i} | D_i = 1, S_i = 1, Y_i \leq y_{q}] - E[Y_{i} | D_i = 0, S_i = 1], \text{ and} \\
& \tau^U_{TB}(1, 1) = E[Y_{i} | D_i = 1, S_i = 1, Y_i \geq y_{1-q}] - E[Y_{i} | D_i = 0, S_i = 1].
\end{aligned}
$$
Here $y_{q}$ is the $q$th-quantile of the observed outcome's conditional distribution in the treatment group. It satisfies the condition $\int_{-\infty}^{y_{q}} dF_{Y|D=1, S=1}(y) = q$, with $F_{Y|D=1, S=1}(\cdot)$ being the distribution function for observed treatment group outcomes.
The quantile $y_{1-q}$ is similarly defined. Justifications for these bounds are provided in Section 3 of the paper.

One limitation of the basic trimming bounds is that they can be very wide in practice and are not informative at all about the LATE for the always-responders. In @samii2023generalizing, we generalize this method by combining it with machine learning techniques to incorporate information from the covariates. We first illustrate how to estimate these bounds conditional on any values of the covariates using the method of *generalized random forests* [@athey2019generalized], which yields bounds for the conditional LATE and may be of independent interest. Then, by integrating these conditional bounds over the covariates distribution of the always-responders, we obtain the "covariate-tightened trimming bounds." They are typically much narrower than the basic ones. Our algorithm uses both Neyman orthogonalization and cross-fitting, following the recommendations of @chernozhukov2018double, such that the estimator does not suffer from any efficiency loss.

##  Simulated data
We now show how to use the main functions in the package with a simulated experiment (*dat_full*). The dataset includes $1,000$ units. The treatment is randomly assigned with a probability of $0.5$. For each unit, we have $10$ covariates, $X1$ to $X10$. All the covariates are uniformed distributed on $[0, 1]$. Among them, only one ($X1$) affects both the outcome and the response. In addition, there exists a variable ($U \sim unif[-2, 2]$) that is unobservable to researchers but also affects both the outcome and the response. We describe the DGP with more details in the [paper](https://papers.ssrn.com/abstract=3555463). 
```{r data, message = FALSE, warning = FALSE, fig.ext = 'png', out.width="60%", , out.height="60%", fig.align = 'center'}
set.seed(1234)
library(grf)
library(ggplot2)
library(CTTB)
data(simData)

N <- nrow(dat_full)

yrange <- range(c(dat_full$Y0, dat_full$Y1))
xrange <- range(dat_full$X2)

par(mfrow = c(1, 1))
plot(Y1 ~ X2, dat_full,
     pch=19,
     col="black",
     ylim=yrange, xlim=xrange,
     main="Potential outcomes",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(Y0 ~ X2, dat_full,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)

plot(subset(dat_full, D==1&S==1)$X2,
     subset(dat_full, D==1&S==1)$Y,
     ylim=yrange, xlim=xrange,
     pch=19,
     col="black",
     main="Experimental outcomes \n (red means attrited)",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(subset(dat_full, D==0&S==0)$X2,
       subset(dat_full, D==0&S==0)$Y,
       ylim=yrange, xlim=xrange,
       pch=19,
       col="red", 
       cex = .4)
points(subset(dat_full, D==1&S==0)$X2,
       subset(dat_full, D==1&S==0)$Y,
       ylim=yrange, xlim=xrange,
       pch=19,
       col="red", 
       cex = .4)
points(subset(dat_full, D==0&S==1)$X2,
       subset(dat_full, D==0&S==1)$Y,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)

plot(subset(dat_full, D==1&S==1)$X2,
     subset(dat_full, D==1&S==1)$Y,
     ylim=yrange, xlim=xrange,
     pch=19,
     col="black",
     main="Observed outcomes",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(subset(dat_full, D==0)$X2,
       subset(dat_full, D==0)$Y,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)
```

Plots above show
```{r estimand, message = FALSE, warning = FALSE}
# generate the data with missing outcome values
dat <- dat_full
dat[dat$S == 0, "Y"] <- NA

cat("The ATE equals ", mean(dat_full$Y1 - dat_full$Y0), "\n")
cat("The ATE for always-responders equals ", mean(true_parameters$Ete_ar), "\n")
cat("The difference-in-means estimator generates an estimate of ", mean(dat$Y[dat$D == 1 & dat$S == 1]) - mean(dat$Y[dat$D == 0 & dat$S == 1]), "\n")
```

## Aggregated covariate-tightened trimming bounds
**CTB** allows users to estimate
We first estimate the aggregated bounds and the classic Lee bounds.
```{r agg, message = FALSE, warning = FALSE}
result <- CTTB(data = dat, seed = NULL, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(2:12)]), W = NULL, Pscore = "Ps", cv_fold = 5, aggBounds = 1, LeeBounds = 1, cBounds = 0, X_moderator = NULL, cond.mono = FALSE)

summary(result)
```

```{r agg-plot, message = FALSE, warning = FALSE}
plot(result)
```

**Uncertainty estimates.** 

**Result summary.** Users can use the **print** function to take a look at a summary of the estimation results or retrieve relevant statistics by directly accessing the fect object. 
Specifically, `est.avg` and `est.avg.unit` show the ATT averaged over all periods -- the former weights each treated observation equally while the latter weights each treated unit equally. 
`est.beta` reports the coefficients of the time-varying covariates. `est.att` reports the average treatment effect on the treated (ATT) by period. Treatment effect estimates from each bootstrap run is stored in `eff.boot`, an array whose dimension = (#time periods * #treated * #bootstrap runs).


## Conditional bounds
```{r cond, message = FALSE, warning = FALSE}
X <- c(names(dat)[c(2:12)])
X_avg <- apply(dat[, X[-2]], 2, mean)
Xm_evals <- quantile(dat[, "X2"], seq(0.05, 0.95, 0.05))
X_moderator <- matrix(rep(X_avg, length(Xm_evals)), length(X_avg), length(Xm_evals))
X_moderator <- rbind(X_moderator, Xm_evals)
X_moderator <- t(X_moderator)
X_moderator <- cbind(X_moderator[, 1], X_moderator[, 11], X_moderator[, 2:10])

# result <- CTB(data = dat, seed = NULL, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(2:12)]), W = NULL, Pscore = "Ps", cv_fold = 5, aggBounds = 1, cBounds = 1, X_moderator = X_moderator, direction = NULL, cond.mono = TRUE)
# 
# tau_l_m_avg <- result[["tau_l_m_avg"]]
# tau_u_m_avg <- result[["tau_u_m_avg"]]
# se_tau_l_m_avg <- result[["se_tau_l_m_avg"]]
# se_tau_u_m_avg <- result[["se_tau_u_m_avg"]]
```




## Application

We now discuss how to replicate the application studies in the paper.

### Santoro and Broockman (2022)

@kalla2022outside studies the effect
of talking with a partner from a different party on attitude toward the other parties. 
The subjects were then randomly assigned into either the treatment group ($D_i = 1$), 
in which they were informed that the partner would be an outpartisan, 
or the control group ($D_i = 0$), in which they received no extra information.
The selection variable is whether the subjects respond to the given question $S_i\in(1,0)$. 

We focus onthe "warmth toward outpartisan voters" outcome (measured by a rescaled thermometer) and
rely on the same covariates the authors selected for their analysis, including the age, gender,
race, education level, and party identification of the subjects, as well as their pre-treatment
outcome.

We present the our CTTB, Lee bounds (conditional monotonic and strict monotonic), and OLS results.

```{r santoro2022 result, message = FALSE, warning = FALSE}
seed <- 42
dat <- get(load("santoro_replic.RData"))


Y <- "post_therm_voter_outparty_rescaled"
S <- "began_convo"
D <- "condition"
X <- names(dat)[c(4, 10, 19:62)]

result <- CTB(data = dat, seed = seed, 
              Y = Y, D = D, S = S, X = X, W = NULL, Pscore = "Ps",
              regression.splitting = FALSE, cv_fold = 5,
              aggBounds = 1,
              cBounds = 0, X_moderator = NULL, 
              direction = NULL,
              cond.mono = 0)

tau_l_est_avg <- result[["tau_l_est_avg"]]
tau_u_est_avg <- result[["tau_u_est_avg"]]
se_tau_l_est_avg <- result[["se_tau_l_est_avg"]]
se_tau_u_est_avg <- result[["se_tau_u_est_avg"]]

tau_l_est_lee_mono <- result[["tau_l_est_lee"]]
tau_u_est_lee_mono <- result[["tau_u_est_lee"]]
se_tau_l_est_lee_mono <- result[["se_tau_l_est_lee"]]
se_tau_u_est_lee_mono <- result[["se_tau_u_est_lee"]]
```

Here `LeeBounds = 1` indicates we want Lee bounds and `cond.mono = 0` indicates we want strict
monotonicity for Lee bounds. 
For our CTTB, the conditional monotonicity is always set as default.


By setting `cond.mono = 1`, we allow the Lee bounds also have conditional monotonicity.

```{r santoro2022 result2, message = FALSE, warning = FALSE}
result.lee <- CTB(data = dat, seed = seed, 
                  Y = Y, D = D, S = S, X = X, W = NULL, Pscore = "Ps",
                  regression.splitting = FALSE, cv_fold = 5,
                  aggBounds = 0,
                  cBounds = 0, X_moderator = NULL, 
                  direction = NULL,
                  cond.mono = 1)

tau_l_est_lee_condmono <- result.lee[["tau_l_est_lee"]]
tau_u_est_lee_condmono <- result.lee[["tau_u_est_lee"]]
se_tau_l_est_lee_condmono <- result.lee[["se_tau_l_est_lee"]]
se_tau_u_est_lee_condmono <- result.lee[["se_tau_u_est_lee"]]

```



```{r santoro2022 result3 , message = FALSE, warning = FALSE}
# estimate the ate using observations that select into the sample
library(sandwich)
reg_formula <- as.formula(paste0(Y, "~", D, "+", paste0(X, collapse = "+")))
tau_reg <- lm(reg_formula, dat[dat[, S] == 1, ])
tau_ate <- coef(tau_reg)[2]
se_tau_ate <- sqrt(vcovHC(tau_reg, type = "HC1")[2, 2])
sqrt(vcovCL(tau_reg, cluster = dat$allsides_link)[2, 2])

all_ests <- c(tau_l_est_avg, tau_u_est_avg, 
              tau_l_est_lee_condmono, tau_u_est_lee_condmono,
              tau_l_est_lee_mono, tau_u_est_lee_mono,
              tau_ate)
all_CI95_upper <- c(tau_l_est_avg + 1.96*se_tau_l_est_avg, 
                    tau_u_est_avg + 1.96*se_tau_u_est_avg, 
                    tau_l_est_lee_condmono + 1.96*se_tau_l_est_lee_condmono, 
                    tau_u_est_lee_condmono + 1.96*se_tau_u_est_lee_condmono,
                    tau_l_est_lee_mono + 1.96*se_tau_l_est_lee_mono, 
                    tau_u_est_lee_mono + 1.96*se_tau_u_est_lee_mono,
                    tau_ate + 1.96*se_tau_ate)
all_CI95_lower <- c(tau_l_est_avg - 1.96*se_tau_l_est_avg, 
                    tau_u_est_avg - 1.96*se_tau_u_est_avg, 
                    tau_l_est_lee_condmono - 1.96*se_tau_l_est_lee_condmono, 
                    tau_u_est_lee_condmono - 1.96*se_tau_u_est_lee_condmono,
                    tau_l_est_lee_mono - 1.96*se_tau_l_est_lee_mono, 
                    tau_u_est_lee_mono - 1.96*se_tau_u_est_lee_mono,
                    tau_ate - 1.96*se_tau_ate)

method <- c("CTB", "CTB", "TB (cond. mono.)", "TB (cond. mono.)", "TB (mono.)", "TB (mono.)", "OLS")
method <- factor(method, levels = c("OLS", "TB (mono.)", "TB (cond. mono.)", "CTB"))

type <- c("Lower", "Upper", "Lower", "Upper", "Lower", "Upper", "ATE")

p.data <- data.frame("Estimates" = round(all_ests, 3), "CI95_u" = round(all_CI95_upper, 3), 
                     "CI95_l" = round(all_CI95_lower, 3), "Method" = method, "Estimand" = type)
 
pd <- position_dodge(width=0.2)
coefs_p <- ggplot(p.data, aes(Method, Estimates, color=Estimand)) +
  geom_point(aes(shape=Method), size=4, position=pd, show.legend = FALSE) +
  scale_color_manual(name="Estimand",values=c("black", "blue", "red")) +
  scale_shape_manual(name="Method",values=c(19, 17, 17, 15)) +
  theme_bw() +
  geom_errorbar(aes(Method, ymin=CI95_l, ymax=CI95_u), width=0.1, position=pd) + 
  geom_hline(aes(yintercept = 0), colour = "black", lty=2) + xlab('') + ylab('') +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(size=25), axis.text.x = element_text(angle = 0), legend.position="top") + 
  ylab("Impact on warmth toward outparty voters") +
  coord_flip()
coefs_p
```

We show how to estimate the conditional bounds. We set the age
as moderator, and keep all other variables at the sample mean. 


```{r santoro 2022 moderate, message = FALSE, warning = FALSE}
X_avg <- c(mean(dat[, X[2]]), c(1, rep(0, 5)), c(0, 1, rep(0, 3)), c(rep(0, 3), 1, 0), c(1, rep(0, 2)), c(rep(0, 10), 1, rep(0, 14)))
Xm_evals <- unique(quantile(dat[, X[1]], seq(0.1, 0.9, 0.1)))
X_moderator <- matrix(rep(X_avg, length(Xm_evals)), length(X_avg), length(Xm_evals))
X_moderator <- rbind(X_moderator, Xm_evals)
X_moderator <- t(X_moderator)
X_moderator <- cbind(X_moderator[, 46], X_moderator[, 1:45])

pf0 <- probability_forest(X = as.matrix(dat[dat[, D] == 0, X]), Y = as.factor(unlist(dat[dat[, D] == 0, S])), seed = seed)
pf1 <- probability_forest(X = as.matrix(dat[dat[, D] == 1, X]), Y = as.factor(unlist(dat[dat[, D] == 1, S])), seed = seed)

pf0_pred <- predict(pf0, newdata = as.matrix(dat[, X]))
pf1_pred <- predict(pf1, newdata = as.matrix(dat[, X]))

pf0_pred_m <- predict(pf0, newdata = as.matrix(X_moderator))
pf1_pred_m <- predict(pf1, newdata = as.matrix(X_moderator))

# estimate Relative share of always-responders
one_q0_m <- pf0_pred_m$predictions[, 2]
one_q1_m <- pf1_pred_m$predictions[, 2]
one_q <- one_q0_m / one_q1_m
one_q[one_q > 1] <- 1/one_q[one_q > 1]

result <- CTB(data = dat, seed = seed,
              Y = Y, D = D, S = S, X = X, W = NULL, Pscore = "Ps",
              regression.splitting = FALSE, cv_fold = 5,
              aggBounds = 0,
              cBounds = 1, X_moderator = X_moderator, 
              direction = NULL,
              cond.mono = 0)

tau_l_m_avg <- result[["tau_l_est_cond"]]
tau_u_m_avg <- result[["tau_u_est_cond"]]
se_tau_l_m_avg <- result[["se_tau_l_est_cond"]]
se_tau_u_m_avg <- result[["se_tau_u_est_cond"]]

```

Setting `cBounds = 1` allows the estimation of conditional bounds.
It needs another argument, `X_moderator`, which is a data frame, for which we estimate
the conditional bounds.

The results are plot as follows:
```{r santoro result plot, message=FALSE, warning=FALSE}
pdata_m <- data.frame("X" = X_moderator[, 1], "tau_u_m_avg" = tau_u_m_avg,
                      "tau_l_m_avg" = tau_l_m_avg, "se_tau_u_m_avg" = se_tau_u_m_avg,
                      "se_tau_l_m_avg" = se_tau_l_m_avg, "one_q" = one_q)
pdata_m <- pdata_m[order(pdata_m$X), ] 
par(mar = c(5, 5, 2, 2))
plot(tau_u_m_avg ~ X, pdata_m, type = "p", ylab = "Conditional bounds", 
     xlab = "Age", col = "red", ylim = c(-.8, 2.2), pch = 20,
     cex = 2, cex.lab=2, cex.axis=2)
lines(tau_u_m_avg - 1.96*se_tau_u_m_avg ~ X, pdata_m, col = "red", lty = 3, lwd = 1)
lines(tau_u_m_avg + 1.96*se_tau_u_m_avg ~ X, pdata_m, col = "red", lty = 3, lwd = 1)
points(tau_l_m_avg ~ X, pdata_m, col = "blue", lty = 2, pch = 20, cex = 2)
lines(tau_l_m_avg - 1.96*se_tau_l_m_avg ~ X, pdata_m, col = "blue", lty = 3, lwd =1)
lines(tau_l_m_avg + 1.96*se_tau_l_m_avg ~ X, pdata_m, col = "blue", lty = 3, lwd = 1)
lines(one_q ~ X, pdata_m, col = "black", lty = 4, lwd = 1)
abline(h = 0, lty = 2, col = "gray", lwd = 1)
legend("topleft", pch = c(20, 20, NA, NA, NA), lty = c(NA, NA, 3, 3, 4), lwd = c(NA, NA, 1, 1, 1),
       col = c("red", "blue", "red", "blue", "black"),
       legend = c("Upper bounds", "Lower bounds", "95% CI of upper bounds", "95% CI of lower bounds", "Relative share of always-responders"), 
       cex = 2, bty = "n")

```

 
---

# Reference
