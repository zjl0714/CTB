---
title: 'Tutorial'
output:
  rmarkdown::html_vignette:
    toc: false
    toc_depth: 4
    number_sections: false
bibliography: cttb.bib      
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
header-includes:  
    - \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    - \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
    - \usepackage{tikz}
    - \usetikzlibrary{positioning,shapes.geometric,chains,calc,shapes}
    - \usepackage{lmodern}
    - \usepackage{amssymb,amsmath}
    - \usepackage{ifxetex,ifluatex}
    - \usepackage{adjustbox} % adjust the size of table
    - \usepackage{mathtools}
    - \usepackage{threeparttable}
    - \usepackage{tabularx}
    - \usepackage{rotating}
    - \usepackage{fixltx2e}
    - \usepackage{bbm}
---
<!-- 
  Code to Justify Text
    <style>
    body {
    text-align: justify}
    </style>
-->   
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```  

In this tutorial, we demonstrate how to use the *CTTB* package in *R* for implementing the method of covariate-tightened trimming bounds proposed by @samii2023generalizing. This method enables empirical researchers to construct sharp bounds for the local average treatment effect (LATE) on a sub-population known as the always-responders in experiments or quasi-experiments, when faced with missing outcome or sample selection driven by potentially unobservable factors. 

# Problems with endogenous sample selection
Consider an experiment with $N$ units. For each unit $i$, we denote the outcome as $Y_i$, the binary treatment status as $D_i$, the binary response indicator as $S_i$, and $P$ pre-treatment covariates as $\mathbf{X}_i = (X_{1i}, X_{2i}, \dots, X_{Pi})$. The outcome $Y_i$ is observed only when $S_i = 1$. The variable $S_i$ reflects whether the unit opts to participate in the experiment or reports their outcome, and its value may be influenced by treatment status. Under the potential outcome framework, we have
$$
Y_i = \begin{cases}
Y_i(1), D_i = 1 \\
Y_i(0), D_i = 0,
\end{cases} \text{ and } S_i = \begin{cases}
S_i(1), D_i = 1 \\
S_i(0), D_i = 0.
\end{cases}
$$
For simplicity, we assume that the treatment is randomly assigned with a positive and known probability, so that $D_i$ is exogenous:
$$
\begin{aligned}
& (Y_i(1), Y_i(0), S_i(1), S_i(0)) \perp D_i, \\
& \varepsilon < P(D_i = 1) < 1 - \varepsilon, \text{ with } \varepsilon > 0.
\end{aligned}
$$
Note that we can estimate the average outcome in either the treatment group or the control group only by conditioning on the event $S_i = 1$. Since $S_i$ is not randomly assigned, the difference-in-means estimator may no longer be unbiased. As depicted in the Directed Acyclic Graph (DAG) below, there may exist an unobservable variable $U_i$ influencing both $S_i$ and $Y_i$. In this case, the difference in the average outcome across the treatment and control groups captures the influence of both $U_i$ and $D_i$, and thus does not estimate the average treatment effect accurately. Since $S_i$ is an endogenous variable in the relationship between $Y_i$ and $(D_i, S_i)$, we refer to this issue as "endogenous sample selection." 
```{tikz, echo=FALSE, fig.ext = 'png', out.width="60%", , out.height="60%", fig.align = 'center'}
\begin{tikzpicture}
[scale=0.7,dot/.style={fill,draw,circle,minimum width=1pt},
arrow style/.style={->,line width=1pt, shorten <=2pt,shorten >=2pt },
arrow2 style/.style={->,line width=2pt, shorten <=2pt,shorten >=2pt },]
\node [fill=white, dot, label=above: $S_{i}$] (s) at (3,3) {}; 
\node [fill=white, dot, label=below right: $Y_{i}$] (y) at (6,1) {} edge [arrow2 style] (s);
\node [fill=white, dot, label=below left: $D_{i}$] (d) at (1,1) {} edge [arrow style] (y) edge [arrow2 style] (s);
\node [fill=black, dot, label=above right: $U_{i}$] (u) at (6,3) {} edge [arrow2 style] (y) edge [arrow2 style] (s);
\end{tikzpicture}
```

This issue is common in social science and can also be interpreted through the lenses of post-treatment bias [@montgomery2018conditioning] or "phantom counterfactual" [@slough2022phantom]. Conventionally, to mitigate the bias arising from conditioning on $S_i$, researchers often turn to sample selection models [@heckman1979sample] or various imputation methods [@honaker2011amelia; @li2013weighting; @blackwell2017unified; @liu2021latent]. However, these methods generally assume that researchers have access to all variables influencing the selection process, a condition that is often unrealistic and contradicts the core rationale of conducting an experiment. 

# Introduction of the method
An alternative approach, which does not rely on such comprehensive knowledge, was proposed by @lee2009training. This method involves bounding the average treatment effect for a specific subgroup known as the always-responders---units whose response indicator remains at $1$ regardless of whether they are under treatment or control conditions. This quantity is formally defined as
$$
\tau(1, 1) = E[\tau_i | S_i(0) = S_i(1) = 1].
$$
It is sometimes referred to as the "intensive margin effect" in economics literature [@staub2014causal; @slough2022phantom]. From a policy standpoint, this particular subgroup represents units for whom the intervention poses minimal concerns about compelling them into an activity they may find undesirable. 

@lee2009training shows that we can bound $\tau(1, 1)$ under a much weaker assumption known as monotonic selection:
$$
S_i(1) \geq S_i(0) \text{ for any unit } i,
$$
which means that the treatment encourages all units to participate in the experiment and report the outcome. Obviously, we can also assume the opposition direction is true. This assumption is implied by all sample selection models but does not require us to know the actual selection process. When it is satisfied, @lee2009training proves that we can construct sharp bounds for $\tau(1, 1)$ as follows:
$$
\begin{aligned}
& \tau^L_{TB}(1, 1) \leq \tau(1, 1) \leq \tau^U_{TB}(1, 1), \text{ with} \\
& \tau^L_{TB}(1, 1) = E[Y_{i} | D_i = 1, S_i = 1, Y_i \leq y_{q}] - E[Y_{i} | D_i = 0, S_i = 1], \text{ and} \\
& \tau^U_{TB}(1, 1) = E[Y_{i} | D_i = 1, S_i = 1, Y_i \geq y_{1-q}] - E[Y_{i} | D_i = 0, S_i = 1].
\end{aligned}
$$
Here $y_{q}$ is the $q$th-quantile of the observed outcome's conditional distribution in the treatment group. It satisfies the condition $\int_{-\infty}^{y_{q}} dF_{Y|D=1, S=1}(y) = q$, with $F_{Y|D=1, S=1}(\cdot)$ being the distribution function for observed treatment group outcomes. The quantile $y_{1-q}$ is similarly defined. This approach is known in the literature as trimming bounds (TB). Theoretical justifications for trimming bounds can be found in Section 3 of @samii2023generalizing.

One limitation of the basic trimming bounds is that they can be quite wide in practice and may provide little useful information about the LATE for the always-responders. @samii2023generalizing extend this method by combining it with a machine learning algorithm, *generalized random forests* or *grf* [@athey2019generalized], to incorporate information from a potentially high-dimensional set of covariates. Using machine learning, researchers can estimate parameters required by the trimming bounds, such as $q$, $y_{q}$, and $y_{1-q}$, for any covariate profile and derive conditional trimming bounds. Then, by integrating these conditional bounds over the covariates distribution of the always-responders, one obtains the "covariate-tightened trimming bounds" (CTTB), which are typically much narrower than the basic trimming bounds. Moreover, the direction of monotonic selection is allowed to vary across covariate profiles. This relaxed assumption---termed conditionally monotonic selection in @semenova2020better---is more realistic in many applications.

The algorithm implemented in the *CTTB* package uses both Neyman orthogonalization and cross-fitting, following the recommendations of @chernozhukov2018double, such that the estimator does not suffer from any efficiency loss and is equipped with honest confidence intervals. The package also offers the option to relax the assumption of monotonic selection for the basic trimming bounds.\footnote{When the assumption of conditionally monotonic selection is imposed, the sample is divided into two subsets based on the direction of monotonic selection. Trimming bounds are then estimated separately within each subset and aggregated to form the final bounds.} Additionally, users can compute confidence intervals for the treatment effect itself, rather than for the bounds, using the method proposed by @imbens2004confidence.

# Simulated data
We now show how to use the main functions in the package with a simulated experiment (*dat_full*). The dataset includes $1,000$ units. The treatment is randomly assigned with a probability of $0.5$ for all units. For each unit, we have $10$ covariates, $X1$ to $X10$. All the covariates are uniformly distributed on $[0, 1]$. Among them, only one ($X1$) affects both the outcome and the response. In addition, there exists a variable ($U \sim unif[-2, 2]$) that is unobservable to researchers but also affects both the outcome and the response. We describe the DGP with more details in the [paper](https://www.cambridge.org/core/journals/political-analysis/article/generalizing-trimming-bounds-for-endogenously-missing-outcome-data-using-random-forests/522453BEDE846D6398CFFA82F5D35A23?utm_date=20250627). The assumption of monotonic selection is satisfied in the simulated data.
```{r data, message = FALSE, warning = FALSE, out.width="60%", , out.height="60%", fig.align = 'center'}
rm(list=ls())

seed <- 2023
set.seed(seed)
library(grf)
library(ggplot2)
library(CTTB)
data(simData)

N <- nrow(dat_full)

yrange <- range(c(dat_full$Y0, dat_full$Y1))
xrange <- range(dat_full$X2)

par(mfrow = c(1, 1))
plot(Y1 ~ X2, dat_full,
     pch=19,
     col="black",
     ylim=yrange, xlim=xrange,
     main="Potential outcomes",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(Y0 ~ X2, dat_full,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)

plot(subset(dat_full, D==1&S==1)$X2,
     subset(dat_full, D==1&S==1)$Y,
     ylim=yrange, xlim=xrange,
     pch=19,
     col="black",
     main="Experimental outcomes \n (red means attrited)",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(subset(dat_full, D==0&S==0)$X2,
       subset(dat_full, D==0&S==0)$Y,
       ylim=yrange, xlim=xrange,
       pch=19,
       col="red", 
       cex = .4)
points(subset(dat_full, D==1&S==0)$X2,
       subset(dat_full, D==1&S==0)$Y,
       ylim=yrange, xlim=xrange,
       pch=19,
       col="red", 
       cex = .4)
points(subset(dat_full, D==0&S==1)$X2,
       subset(dat_full, D==0&S==1)$Y,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)

plot(subset(dat_full, D==1&S==1)$X2,
     subset(dat_full, D==1&S==1)$Y,
     ylim=yrange, xlim=xrange,
     pch=19,
     col="black",
     main="Observed outcomes",
     xlab=expression('X'[1]), ylab="Y", 
     cex.lab=1, cex.axis=1, cex = .4)
points(subset(dat_full, D==0)$X2,
       subset(dat_full, D==0)$Y,
       pch=21,
       col="black",
       bg="white", 
       cex = .4)
```

The first two plots above display the distributions of the potential outcomes without and with sample attrition, respectively, while the last plot shows the distribution of observed outcomes in the data. Due to endogenous sample selection, the difference-in-means estimator is biased for either the ATE or the LATE on the always-responders.
```{r estimand, message = FALSE, warning = FALSE}
# generate the data with missing outcome values
dat <- dat_full
dat[dat$S == 0, "Y"] <- NA
ate <- mean(dat_full$Y1 - dat_full$Y0)
late_ar <- mean(true_parameters$Ete_ar)
cat("The ATE equals ", ate, "\n")
cat("The LATE for always-responders equals ", late_ar, "\n")
cat("The difference-in-means estimator generates an estimate of ", mean(dat$Y[dat$D == 1 & dat$S == 1]) - mean(dat$Y[dat$D == 0 & dat$S == 1]), "\n")
```

# Aggregated trimming bounds
We first use the *CTTB()* function to estimate the aggregated bounds, including the basic trimming bounds (*LeeBounds = TRUE*) and the covariate-tightened trimming bounds, for the LATE on the always-responders in the data. The function requires users to specify the dataset ($data$), the outcome variable ($Y$), the treatment ($D$), the response indicator ($S$), the covariates ($X$), and the propensity score ($Ps$). In observational studies, propensity score does not need to be specified and will be estimated from data (see the application below). We allow for the possibility of conditionally monotonic selection (*cond\_mono = TRUE*) when estimating the basic trimming bounds and construct confidence intervals for both the effect estimate and the bounds (*IM\_cv = TRUE*).

By applying the *summary()* function to the output of *CTTB()*, researchers can view the estimated lower and upper bounds using either TB or CTTB, along with their standard error estimates and 95\% confidence intervals for both the bounds and the treatment effect---all presented in data frame format. Relevant information can then be easily extracted for further analysis. The results suggest that bounds estimated by either CTTB or TB cover the LATE for the always-responders, consistent with the theoretical conclusion. In addition, the identified set estimated by CTTB---the range between the lower and the upper bound---are indeed narrower than those obtained using TB, and they indicate that the LATE for the always-responders is significantly greater than zero in the simulated data.
```{r agg, message = FALSE, warning = FALSE}
result <- CTTB(data = dat, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(3:12)]), Pscore = "Ps", LeeBounds = TRUE, IM_cv = TRUE, cond_mono = TRUE)

summary(result)
```

```{r agg-plot, message = FALSE, warning = FALSE, out.width="60%", , out.height="60%", fig.align = 'center'}
p <- plot(result) 
p
```
Researchers can also visualize results from *CTTB()* using the *plot()* function, which produces a coefficient plot with confidence intervals based on *ggplot2*. Additional elements can be added to the generated plot to support further analysis. For example, one can compare the estimated bounds against the ATE and the LATE for the always-responders, as illustrated in the plot below.
```{r agg-plot-truth, message = FALSE, warning = FALSE, out.width="60%", , out.height="60%", fig.align = 'center'}
p + geom_hline(aes(yintercept = ate), colour = "red", lty=2) +
  geom_hline(aes(yintercept = late_ar), colour = "red", lty=4)
```

There are several additional options that researchers can control. *seed* is the random seed used by functions in the *grf* package. *cv\_fold* determines the number of folds used in cross-fitting (default is 5). *splitting* option specifies whether regression splitting is enabled when estimating conditional quantiles (default is *FALSE*).\footnote{Refer to the help file for *quantile\_forest()* in the *grf* package for more details.} *trim_l* and *trim_u* allow researchers to truncate propensity score or conditional trimming probability estimates (default values are 0 and 1): values below *trim_l* (or above *trim_u*) are replaced by *trim_l* (or *trim_u*). *alpha* is the significance level for confidence intervals (default is 0.05). In the example below, we set *cv\_fold = 10*,  *splitting = TRUE*, *trim_l = 0.1*, and *trim_u = 0.9*. The results remain largely unchanged.
```{r agg-options, message = FALSE, warning = FALSE, out.width="60%", , out.height="60%", fig.align = 'center'}
result <- CTTB(data = dat, seed = NULL, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(3:12)]), Pscore = "Ps", cv_fold = 10, splitting = TRUE, trim_l = 0.1, trim_u = 0.9,  LeeBounds = TRUE, cond_mono = FALSE)

plot(result)
```


# Conditional trimming bounds
With *CTTB()*, researchers can also examine how the trimming bounds vary across covariate profiles, which is referred to as conditional trimming bounds by @samii2023generalizing. When *cBounds = TRUE*, the function estimates these bounds across covariate profiles, where the moderator specified by the user ($M$) varies in 5-percentile increments and all other covariates are fixed at their sample means (or modes for dichotomous variables).
```{r cond, message = FALSE, warning = FALSE}
result <- CTTB(data = dat, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(3:12)]), Pscore = "Ps", aggBounds = FALSE, cBounds = TRUE, M = "X2")

summary(result)
```
In the simulated data, we estimate the conditional trimming bounds with $X_1$ being the moderator. Researchers can use the *summary()* function to inspect the estimates or the *plot()* function to visualize them by setting *type = "conditional"*. The plot below illustrates how the estimated bounds and their pointwise 95\% confidence intervals vary with $X_1$. Notably, the lower bound estimates become significantly greater than zero when $X_1$ exceeds 0.38.
```{r cond-plot, message = FALSE, warning = FALSE, out.width="60%", , out.height="60%", fig.align = 'center'}
plot(result, type = "conditional")
```


# Application
We now demonstrate the method's performance using empirical studies analyzed in @samii2023generalizing.

## Santoro and Broockman (2022)
@santoro2022promise conduct an online experiment to examine the effect of talking with an outpartisan---someone from a different political party---on attitudes toward the opposing parties. Subjects were randomly assigned to either the treatment group ($D_i = 1$), in which they were informed that their partner would be an outpartisan, or the control group ($D_i = 0$), in which no such information was provided. $S_i \in {0, 1}$ indicates whether a subject initiated the online conversation and completed the questionnaire. Always-responders in this study are those who would engage in the conversation regardless of their partner's partisanship.

We focus on the outcome of "warmth toward outpartisan voters" measured by a feeling thermometer and the following covariates: age, gender,
race, education level, party identification, and the outcome's pre-treatment level. We first estimate the aggregated bounds, using both TB and CTTB. 
```{r santoro2022 result, message = FALSE, warning = FALSE}
rm(list=ls())

seed <- 2023
set.seed(seed)
library(CTTB)
library(grf)
library(ggplot2)

dat <- get(load("santoro_replic.RData"))

Y <- "post_therm_voter_outparty_rescaled"
S <- "began_convo"
D <- "condition"
X <- names(dat)[c(4, 10, 19:62)]

result <- CTTB(data = dat, seed = seed, Y = Y, D = D, S = S, X = X, 
               W = NULL, Pscore = "Ps", LeeBounds = TRUE)

summary(result)
```

```{r santoro2022-plot, message = FALSE, warning = FALSE}
p <- plot(result)
p
```

The results are shown in the plot above. The identified set from the CTTB method is again narrower than that from the TB method. The lower bound is significantly greater than zero at the 5\% level, suggesting that the LATE for the always-responders is positive. In the plot below, we compare the estimated bounds with the OLS estimate. The OLS estimate falls outside the identified set produced by CTTB in this case.
```{r santoro2022 result3 , message = FALSE, warning = FALSE}
# estimate the ate using observations that select into the sample
library(sandwich)
reg_formula <- as.formula(paste0(Y, "~", D, "+", paste0(X, collapse = "+")))
tau_reg <- lm(reg_formula, dat[dat[, S] == 1, ])
tau_ate <- coef(tau_reg)[2]
se_tau_ate <- sqrt(vcovHC(tau_reg, type = "HC1")[2, 2])

p.data <- data.frame("Estimate" = round(tau_ate, 3), "CI95_upper" = round(tau_ate + 1.96*se_tau_ate, 3), "CI95_lower" = round(tau_ate - 1.96*se_tau_ate, 3), "Method" = "OLS")

p <- p + geom_point(data = p.data, aes(x = Method, y = Estimate), shape = 17, size = 3, color = "grey5") + 
  geom_errorbar(data = p.data, aes(x = Method, ymin = CI95_lower, ymax = CI95_upper), color = "grey5", width = 0.2)
p
```
Next, we estimate the conditional bounds using age as the moderator. The results suggest that the effect may be larger among younger subjects.
```{r santoro 2022 moderator, message = FALSE, warning = FALSE}
result <- CTTB(data = dat, seed = seed, Y = Y, D = D, S = S, X = X, Pscore = "Ps", aggBounds = FALSE, cBounds = TRUE, M = "age")

p <- plot(result, type = "conditional")
p
```


## Blattman and Annan (2010)
@blattman2010consequences is an observational study examining the long-term effects of being abducted into a rebel organization as a youth ($D_i$) in Northern Uganda. The authors surveyed a random sample of households to measure outcomes such as education, psychological distress, and wages among former abductees. Some selected respondents never returned from abduction, while others could not be located by the research team. We focus on psychological distress ($Y_i$) as the outcome of interest and use $S_i$ to indicate whether a respondent was successfully interviewed. Covariates are drawn from the original analysis and include only variables without missing values, such as the respondent's age and location and the household's wealth level.

Results from both CTTB and TB are reported below. We apply the original sampling weights ($W_i$) and use 10-fold cross-validation. Estimates of the conditional trimming probability are truncated at 0.1 from below and 0.9 from above. The lower bound estimate is significantly greater than zero, suggesting that abduction has a persistent impact on psychological distress.
```{r BA 2010 main, message = FALSE, warning = FALSE}
rm(list=ls())

seed <- 2023
set.seed(seed)
library(CTTB)
library(grf)
library(ggplot2)

dat <- get(load("blattman_replic.RData"))

Y <- "distress"
S <- "found"
D <- "abd"
W <- "i_weight"
AC <- c("C_ach", "C_akw", "C_ata", "C_kma", "C_oro", "C_pad", "C_paj", "A14", "A15", "A16", "A17", "A18", "A19", "A20", "A21", "A22", "A23", "A24", "A25", "A26", "A27", "A28", "A29") # age dummies and race dummies
ACG <- c(AC, "G1_18_21", "G1_22_25", "G2_14_17", "G2_22_25", "G3_14_17", "G3_18_21", "G4_14_17", "G4_18_21", "G4_22_25", 
         "G5_14_17", "G5_18_21", "G5_22_25", "G6_14_17", "G6_18_21", "G6_22_25", "G7_14_17", "G7_18_21", "G7_22_25", "G8_14_17", "G8_18_21", "G8_22_25")
X <- c("hh_fthr_frm", "hh_size96", "hh_land", "hh_cattle", "hh_stock", "hh_plow", "age", ACG)

result <- CTTB(data = dat, seed = seed, Y = Y, D = D, S = S, X = X, W = W, cv_fold = 10, LeeBounds = TRUE, trim_l = 0.05, trim_u = 0.95)

summary(result)
```
```{r BA 2010 plot, message = FALSE, warning = FALSE}
p <- plot(result)
p
```
 
<!-- # Kalla and Broockman (2022)  -->
<!-- @kalla2022outside is -->

---

# Reference
