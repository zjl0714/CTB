\encoding{UTF-8}
\name{print.CTTB}
\alias{print.CTTB}
\title{Print results for "CTTB" objects}
\description{Print results generated by the \code{CTTB} method.}

\usage{\method{print}{CTTB}(result, ...)
}

\arguments{
\item{CTTB}{a generated \code{CTTB} object.}
}

\value{
  Data frames storing results from the \code{CTTB} function.
}

\author{
  Cyrus Samii (\email{cds2083@nyu.edu}), New York University.

  Ye Wang (\email{yewang@unc.edu}), UNC-Chapel Hill.

  Aaron Junlong Zhou (\email{jlzhou@nyu.edu}), Tecent America
}

\references{
  Athey, Susan, Julie Tibshirani, and Stefan Wager. (2019).
  Generalized Random Forests.
  \emph{The Annals of Statistics, 47}(2): 1148–78.

  Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. (2018).
  Double/Debiased Machine Learning for Treatment and Structural Parameters.
  \emph{The Econometrics Journal, 21}(1): C1–68.

  Lee, David S. (2009).
  Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects.
  \emph{The Review of Economic Studies, 76}(3): 1071–1102.

  Samii, Cyrus, Ye Wang, and Junlong Aaron Zhou. (2025).
  Generalizing Trimming Bounds for Endogenously Missing Outcome Data Using Random Forests.
  \emph{Political Analysis, First View}: 1-15.
}

\examples{
library(CTB)
data(simData)
out <- CTB(data = dat, seed = 1234, Y = "Y", D = "D", S = "S", X = c(names(dat)[c(2:(P+2))]),                       Pscore = "Ps", regression.splitting = FALSE, cv_fold = 5, aggBounds = TRUE,
           cBounds = TRUE, X_moderator = X_moderator, direction = NULL, cond.mono = 0)
print(out)
}


